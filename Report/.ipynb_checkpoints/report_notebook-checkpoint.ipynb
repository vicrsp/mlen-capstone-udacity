{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Victor São Paulo Ruela\n",
    "\n",
    "February 5th, 2019\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "Partial Discharge (PD) signals are electrical discharges that can occur inside the insulation of high voltage equipments. These signals have a repetitive nature and are confined to small regions, which in the long run can lead to irreparable damage to the equipment. Therefore, it is vital for the energy industry companies to monitor the occcurrance of PDs, in order to prevent accidents and guarantee a realiable energy transmission for its customers.\n",
    "\n",
    "Measuring these signals in the field is already very challenging task, due to its low intensity and high noise levels from high voltage systems. However, this is just one part of the problem: based on the measurements, how can we predict some of its characteristics, such as faults, the level of damage, local of occurrence and possible causes? These are tasks that can be achieve with signal processing techniques and machine learning algorithms.\n",
    "\n",
    "The dataset that is going to be used is available from the VSB Power Line Fault Detection Kaggle competition website:\n",
    "\n",
    "https://www.kaggle.com/c/vsb-power-line-fault-detection\n",
    "\n",
    "It contains several examples of labeled PD signals (fault or undamaged). Each signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds for each one of the three phases.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The goal is to train a classification algorithm to predict for PD signal measurements if a power line damaged or not. I will be tackling this problem as a binary classification problem, also applying digital signal processing techniques to extract the most relevant features from the PD signals.\n",
    "\n",
    "The first task will be to apply signal processing techniques in order to remove the background noise from the measurements and obtain a clear representation of the partial discharge signals. This can be done using digital filters (Butterworth, Chebyschev, etc.), the Fourier Transform and the Discrete Wavelet Transform (DWT). After that, new features will be extracted, such as amount of PDs and the frequency-domain content. If necessary, more features can be include based on the thesis from the competition's responsible [1].\n",
    "\n",
    "For the training models, I pretend to compare binary classification models, such as Logistic Regression and Random Forests. I will work with simpler models in order to have more time available for the feature extraction, since this should be the most important taks on this porject. I expect to spend 70% of the time on the signal processing and feature extraction parts and 30% of the time on training models and tweaking parameters.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "The results will be evaluated with the [Matthews correlation coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) between the predicted and the observed response:\n",
    "\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7 \"MCC\")\n",
    "\n",
    "where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives. This is a very suitable metric, since the dataset is probably unbalanced because faults are not very frequent. \n",
    "\n",
    "This is the same metric used in the competition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "_(approx. 2-4 pages)_\n",
    "\n",
    "### Data Exploration and Visualization\n",
    "\n",
    "The dataset is available at https://www.kaggle.com/c/vsb-power-line-fault-detection/data\n",
    "\n",
    "It contains the following files:\n",
    "\n",
    "a) metadata_[train/test].csv - The signal general information and labels. Contatins the following columns:\n",
    "\n",
    "* `id_measurement`: the ID code for a trio of signals recorded at the same time.\n",
    "\n",
    "* `signal_id`: the foreign key for the signal data. Each signal ID is unique across both train and test, so the first ID in train is '0' but the first ID in test is '8712'.\n",
    "\n",
    "* `phase`: the phase ID code within the signal trio. The phases may or may not all be \n",
    "impacted by a fault on the line.\n",
    "\n",
    "* `target`: 0 if the power line is undamaged, 1 if there is a fault.\n",
    "\n",
    "b) [train/test].parquet - The signal data. Each column contains one signal; 800,000 int8 measurements as exported with pyarrow.parquet version 0.11. More information about the parquet format can be found in https://acadgild.com/blog/parquet-file-format-hadoop.\n",
    "\n",
    "The metadata file is the link between the actual PD signals measurements in the parquet file and its label. For example, to access the signal with ID \"1\", you just need to load the column \"1\" from the .parquet file. This is actually a very interesting feature, because it allows to optimize the memory consumption by processing one signal at a time.\n",
    "\n",
    "The `metadata_train.csv` file contains `8712` rows, each one representing a signal available in the `train.parquet` file. The overall and per phase distribution for the labels are displayed in the figures below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!---\n",
    "![Label distribution - per phase](train_data_dist.png \"Label distribution - per phase\")\n",
    "\n",
    "![Label distribution - Overall](train_data_dist_targets.png \"Label distribution - Overall\")\n",
    "\n",
    "-->\n",
    "\n",
    "![Label distribution - Overall](train_data_dist_twosided.png \"Label distribution - Overall\")\n",
    "\n",
    "\n",
    "We can clearly see the data is very unbalanced: only `6.0262%` of the data contains examples of fault signals. This was expected, since a fault event is not very common. Looking at the distribution considering the phase of the signal, we can see that the labels are evenly distributed among them. Therefore, phase should not be an input variable to the models, since it won't add any relevante information.\n",
    "\n",
    "Eah signal in the `train.parquet` contains `800,000` measurements of a power line's voltage, taken over `20` milliseconds. Thefore, the sampling rate is `40Mhz`. A sample signal PD signal for each phase can be seen in the figure below.\n",
    "\n",
    "![PD Signal](signal_phase_example.png \"PD Signal\")\n",
    "\n",
    "The figure below shows a comparison between a fault and normal example signals.\n",
    "![Fault and normal signals](signal_fault_normal_raw.png \"Fault and normal signals\")\n",
    "\n",
    "Based on these examples, the following can be inferred about PD signals:\n",
    "\n",
    "* There is a lot of background noise, which can be incorrectly identified as a PD pattern. Therefore, it is necessary to denoise this signal ass the first step for the analysis.\n",
    "* It's not easy to visually observe an actual PD pattern, since it happens very fast.\n",
    "* The start and end of the measuremente cycle is different depending on the phase.\n",
    "\n",
    "Moreover, the raw time-based data format should not be used for the machine learning algorithms, since this would lead to have `800,000` features as inputs to the model.Therefore, signal processing techniques will have to applied to extract features that can accurately represent the signal.\n",
    "\n",
    "\n",
    "### Algorithms and Techniques\n",
    "\n",
    "\n",
    "\n",
    "In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:\n",
    "- _Are the algorithms you will use, including any default variables/parameters in the project clearly defined?_\n",
    "- _Are the techniques to be used thoroughly discussed and justified?_\n",
    "- _Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?_\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "The benchmark model will be a naive predictor that will output that the power line is always undamaged (false). Based on the data discussion from previous sections, this model will have `93.9738%` accuracy. This model will also have a baseline value of `0.0` for the Mathews Correlation Coefficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "_(approx. 3-5 pages)_\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "As discussed in the previous sections, signal processing techniques must be applied in order to extract the most relevant information from the signals. The first step is applying a filter to the signal in order to remove the background noise. An analysis of the most common PD denoising techniques is available in [2]. For this project, a high pass filter and a Discrete Wavelet Transform (DWT) denoising technique will be used.\n",
    "\n",
    "The high pass filter will remove all frequency content below a certain threshold. This is very important to remove the 50Hz baseline frequency from the power line, and also some low frequency noise. The threshold that is going to be used is 10kHz, since PD patterns are only observed above this value.\n",
    "\n",
    "The DWT is the most used signal processing technique for PD denoising, having several papers studying its application available in the literature. It works by decomposing the signal in several scales and levels based on a pre-selected mother Wavelet. The framework for PD denoising consists in:\n",
    "\n",
    "* Apply the DWT to the noisy signal until a decomposition level where its possible to distinguish the DP signal. This will calculate the wavelet coefficients $c$ for each level.\n",
    "* Chosse an appropriate threshold $T_{d}$ for selecting the coefficients for each level. This can be done either with soft or hard thresholding.\n",
    "* Recover the denoised signal using the inverse discrete Wavelet transform from the coefficients selected previously.\n",
    "\n",
    "A more detailed description regarding the Wavelet denoising framework can be found in [1,3]. For this project, the a hard-thresholding approach from [1] will be used. The limit $T_{d,j}$ is calculated based on the Mean Absolute Deviation (MAD), using the following equations: \n",
    "\n",
    "$$\\sigma = \\frac{1}{0.6745}~MAD(|c|) $$\n",
    "\n",
    "$$T_{d} = \\sigma\\sqrt{2\\log{n}} $$\n",
    "\n",
    "where $n$ is the lenght of the signal. The selected mother wavelet will be the `Debauchy 4`, since it was indicated in [2] as the most similar to the PD pattern.\n",
    "\n",
    "The high pass filter and DWT denoising will be applied for every signal in the database. After this step, we can start extracting new features to represente this signal. An filtered signal example can be seen in the figure below.\n",
    "\n",
    "![PD Signal Denoised](signal_phase_denoised.png \"PD Signal Denoised\")\n",
    "\n",
    "Afther that, time and frequency domain features will be extracted from the denoised signal. According to [4], the most relevant time domain features are:\n",
    "\n",
    "* Number of peaks\n",
    "* Mean width of peaks\n",
    "* Max width of peaks\n",
    "* Min width of peaks\n",
    "* Mean height of peaks\n",
    "* Max height of peaks\n",
    "* Min height of peaks\n",
    "\n",
    "The signal will be divided into 4 sections of size `200,000` and these features will be extracted for each subset, as suggested in [4]. This is done because the appearance of peaks are not random, i.e, they are clustered in specific subparts of the sinusoidal signal. This yields a total of 28 features extracted.\n",
    "\n",
    "In order to extract these features, the functions [find_peaks](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html) and [peak_widths](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_widths.html#scipy.signal.peak_widths) from the `scipy` package will be used. It was necessary to fine tune the input parameters for both functions, and after several trials the following parameters were selected: \n",
    "\n",
    "1. **find_peaks**\n",
    "    * prominence: `10` (The prominence of a peak measures how much a peak stands out from the surrounding baseline of the signal and is defined as the vertical distance between the peak and its lowest contour line.)\n",
    " \n",
    "    * distance: `50` (Required minimal horizontal distance in samples between neighbouring peaks) \n",
    "\n",
    "2. **peak_widths**\n",
    "    * rel_height: `0.9` (Relative height at which the peak width is measured as a percentage of its prominence)\n",
    "\n",
    "In the following figures, it is possible to see the peaks from an example signal and also have a closer look at the width and height of a peak.\n",
    "\n",
    "![Signal Peaks](signal_peaks.png \"Signal Peaks\")\n",
    "\n",
    "![Peak height and width](signal_peak_zoom.png \"Peak height and width\")\n",
    "\n",
    "The frequency domain features are based on the power spectral density (PSD) of each denoised signal. The PSD will not be calculated based on the raw signal because the noise contamination in higher frequencies will introduce undesired bias to the data. The PSD will be split into 4 equals sections of 5 MHz (Nyquist frequency divided by 4), such that we can extract the following features from the signal at different frequency ranges:\n",
    "\n",
    "* Sum of power spectrum\n",
    "* Maximum value\n",
    "* Frequency of maximum value \n",
    "* Mean value\n",
    "\n",
    "This will be calculated with the [welch](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.welch.html) function from the `scipy` package. Welch's method calculates an estimate of the power spectral density by dividing the data into overlapping segments, computing a modified periodogram for each segment and averaging the periodograms. An example for a faulted and normal signal can be seen in the figure below. This task yields more 16 extracted features.\n",
    "\n",
    "![Power spectral density](signal_fault_normal_psd_denoised.png \"Power spectral density\")\n",
    "\n",
    "After applying these two feature extraction, we finally have the dataset that is going to be used for training our prediction models. This dataset has 44 numerical features for each signal and their corresponding labels. The statistical description can be seen in the table below.\n",
    "\n",
    "It can be seen that the features have very different magnitudes, therefore they will be normalized to the 0-1 range. The following figure shows the correlation matrix between the features. \n",
    "\n",
    "\n",
    "In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:\n",
    "- _If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?_\n",
    "- _Based on the **Data Exploration** section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?_\n",
    "- _If no preprocessing is needed, has it been made clear why?_\n",
    "\n",
    "### Implementation\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "- _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_\n",
    "- _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_\n",
    "- _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_\n",
    "\n",
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- _Has an initial solution been found and clearly reported?_\n",
    "- _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- _Are intermediate and final solutions clearly reported as the process is improved?_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "- _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- _Can results found from the model be trusted?_\n",
    "\n",
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "- _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- _Is the final solution significant enough to have solved the problem?_\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- _Is the visualization thoroughly analyzed and discussed?_\n",
    "- _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- _Were there any interesting aspects of the project?_\n",
    "- _Were there any difficult aspects of the project?_\n",
    "- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_\n",
    "\n",
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- _If you used your final solution as the new benchmark, do you think an even better solution exists?_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "\n",
    "[2] S. Sriram, S. Nitin, K.M.M. Prabhu, and M.J. Bastiaans. Signal denoising techniques for\n",
    "partial discharge measurements. IEEE Transactions on Dielectrics and Electrical Insulation,\n",
    "12(6):1182–1191, 2005\n",
    "\n",
    "[3] Ma, X., Zhou, C. and Kemp, I.J., 2002. Interpretation of wavelet analysis and its application in partial discharge detection. IEEE Transactions on Dielectrics and Electrical Insulation, 9(3), pp.446-457.\n",
    "\n",
    "[4] S. Misák, J. Fulnecek, T. Vantuch, T. Buriánek and T. Jezowicz, \"A complex classification approach of partial discharges from covered conductors in real environment,\" in IEEE Transactions on Dielectrics and Electrical Insulation, vol. 24, no. 2, pp. 1097-1104, April 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
